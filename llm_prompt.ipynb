{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54090b40",
   "metadata": {},
   "source": [
    "# Final: LLM Prompt Checkpoint\n",
    "\n",
    "```{tip}\n",
    "This is a great time to use few-shot prompting.\n",
    "\n",
    "Structured outputs are probably overkill since we just want a string\n",
    "```\n",
    "\n",
    "1. Design a LLM system prompt for converting a sentence requesting weather into the format `wttr.in` needs.\n",
    "2. Design several test cases.\n",
    "3. Evaluate your prompt; iterate if necessary.\n",
    "4. Upload this completed notebook to Gradescope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679311f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4221a",
   "metadata": {},
   "source": [
    "## Template\n",
    "\n",
    "Complete this and submit to Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4be6a5",
   "metadata": {},
   "source": [
    "### LLM Setup and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script evaluates an LLM prompt for processing text so that it can be used for the wttr.in API\"\"\"\n",
    "\n",
    "from ollama import Client\n",
    "\n",
    "LLM_MODEL: str = \"gemma3:1b\"  # Change this to be the model you want\n",
    "client: Client = Client(\n",
    "    host=\"http://localhost:11434\"  # Change this to be the URL of your LLM\n",
    ")\n",
    "\n",
    "few_shot_prompt: str = \"\"\"\n",
    "Given a weather query, return a formatted string that includes the location for the wttr.in API. This API takes the following formats:\n",
    "\n",
    "1. Cities\n",
    "2. 3-letter airport codes\n",
    "\n",
    "If a popular attraction/geographical location is mentioned, output the city that the attraction is located in. \n",
    "Anytime the city is more than one word, replace the spaces between the words with '+' and capitalize all words. For 3-letter airport codes, all three letters are lowercase.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: What is the weather in Vegas?\n",
    "Output: Las+Vegas\n",
    "\n",
    "Input: What's the weather near the Eiffel Tower?\n",
    "Output: Paris\n",
    "\n",
    "Input: Please give me the weather in HNL.\n",
    "Output: Honolulu\n",
    "\n",
    "Input: I'm at JFK right now. What's the weather?\n",
    "Output: New+York+City\n",
    "\n",
    "Input: What's the weather in New York City?\n",
    "Output: New+York+City\n",
    "\n",
    "Input: Forecast for Rio Rancho?\n",
    "Output: Rio+Rancho\n",
    "\n",
    "\n",
    "Please note how New York City and Las Vegas are each two or more words, so the output includes a '+' where the space between the words should be.\n",
    "These two examples mentioned above are not the only instances where a '+' is necessary. This rule applies to ANY city that is more than one word.\n",
    "Therefore, do not give me an output with any spaces. There are '+' instead of spaces in the output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# TODO: define llm_parse_for_wttr()\n",
    "def llm_parse_for_wttr(prompt: str) -> str:\n",
    "    response = client.chat(\n",
    "        messages= [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": few_shot_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=LLM_MODEL,\n",
    "    )\n",
    "\n",
    "    \n",
    "    output = response[\"message\"][\"content\"].strip() #used AI for this line\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762c389",
   "metadata": {},
   "source": [
    "### Test cases and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8fb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_cases = [  # TODO: Replace these test cases with ones for wttr.in\n",
    "    {\"input\": \"What's the weather in Rio Rancho?\", \"expected\": \"Rio+Rancho\"},\n",
    "    {\"input\": \"What's the weather in PHX?\", \"expected\": \"Phoenix\"},\n",
    "    {\"input\": \"What's the weather in New York City?\", \"expected\": \"New+York+City\"},\n",
    "    {\"input\": \"What's the weather at the Eiffel Tower? \", \"expected\": \"Paris\"},\n",
    "    {\"input\": \"Give me the weather in Colorado Springs\", \"expected\": \"Colorado+Springs\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a19725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(test_cases: list[dict[str, str]]):\n",
    "    \"\"\"run_tests iterates through a list of dictionaries,\n",
    "    runs them against an LLM, and reports the results.\"\"\"\n",
    "    num_passed = 0\n",
    "\n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        raw_input = test[\"input\"]\n",
    "        expected_output = test[\"expected\"]\n",
    "\n",
    "        print(f\"\\nTest {i}: {raw_input}\")\n",
    "        try:\n",
    "            result = llm_parse_for_wttr(raw_input).strip()\n",
    "            expected = expected_output.strip()\n",
    "\n",
    "            print(\"LLM Output  :\", result)\n",
    "            print(\"Expected    :\", expected)\n",
    "\n",
    "            if result == expected:\n",
    "                print(\"‚úÖ PASS\")\n",
    "                num_passed += 1\n",
    "            else:\n",
    "                print(\"‚ùå FAIL\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"üí• ERROR:\", e)\n",
    "\n",
    "    print(f\"\\nSummary: {num_passed} / {len(test_cases)} tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53e030",
   "metadata": {},
   "source": [
    "### Execute tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f14e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1: What's the weather in Rio Rancho?\n",
      "LLM Output  : Rio+Rancho\n",
      "Expected    : Rio+Rancho\n",
      "‚úÖ PASS\n",
      "\n",
      "Test 2: What's the weather in PHX?\n",
      "LLM Output  : Phoenix\n",
      "Expected    : Phoenix\n",
      "‚úÖ PASS\n",
      "\n",
      "Test 3: What's the weather in New York City?\n",
      "LLM Output  : New+York+City\n",
      "Expected    : New+York+City\n",
      "‚úÖ PASS\n",
      "\n",
      "Test 4: What's the weather at the Eiffel Tower? \n",
      "LLM Output  : Paris\n",
      "Expected    : Paris\n",
      "‚úÖ PASS\n",
      "\n",
      "Test 5: Give me the weather in Colorado Springs\n",
      "LLM Output  : Colorado Springs\n",
      "Expected    : Colorado+Springs\n",
      "‚ùå FAIL\n",
      "\n",
      "Summary: 4 / 5 tests passed.\n"
     ]
    }
   ],
   "source": [
    "run_tests(test_cases=test_cases)\n",
    "#using 1b model, will likely be more accurate with a larger model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
